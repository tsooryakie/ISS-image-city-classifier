"""
This program performs hyperparameter tuning by Randomly Searching a randomly generated hyperparameter space.
The hyperparameter space is defined and bounded by sensible and expected values which were empirically
observed from previous experiments.
Version: 10/08/2020
"""
import sys
from typing import Dict

import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim

from model_training import (
    load_dataset_and_transforms,
    initialise_model,
    get_parameters_to_learn,
    train_model,
)


def generate_hyperparameters(num_loops: int) -> Dict:
    """
    This function generates the hyperparameter space, which is then used in evaluation to evaluate
    different hyperparameters and their effect on the defined performance metric (in this case, validation accuracy).
    :param num_loops: Number of hyperparameter loops to test
    :return:
    """
    learning_rates = [
        np.round(np.random.uniform(0.00005, 0.00015), 9)
        for learning_rate in range(num_loops)
    ]
    print("Smallest Learning Rate: " + str(np.min(learning_rates)))
    print("Largest Learning Rate: " + str(np.max(learning_rates)))
    batch_sizes = [np.random.randint(60, 70) for batch_size in range(num_loops)]
    print("Smallest Batch Size: " + str(np.min(batch_sizes)))
    print("Largest Batch Size: " + str(np.max(batch_sizes)))
    weight_decays = [
        np.round(np.random.uniform(0.0005, 0.001), 9)
        for weight_decay in range(num_loops)
    ]
    print("Smallest Weight Decay: " + str(np.min(weight_decays)))
    print("Largest Weight Decay: " + str(np.max(weight_decays)))

    hyperparameters = {
        "learning_rates": learning_rates,
        "batch_sizes": batch_sizes,
        "weight_decays": weight_decays,
    }

    fig = plt.figure(figsize=(10, 7))
    ax = fig.add_subplot(111, projection="3d")
    hp_3d_plot = ax.scatter3D(
        learning_rates,
        batch_sizes,
        weight_decays,
        c=weight_decays,
        cmap="jet",
        marker="o",
        s=25,
    )
    colourbar = plt.colorbar(hp_3d_plot)
    ax.set_xlabel("Learning Rates")
    ax.set_ylabel("Batch Sizes")
    ax.set_zlabel("Weight Decays")
    plt.title("Randomly Generated Hyperparameter Space")
    plt.savefig("../visualisations/hyperparameter_space/hyperparameter_space.png")

    return hyperparameters


def set_up_training_loops(model_name: str, hyperparameter_dict: Dict) -> None:
    """
    This function performs the training with given generated hyperparameters for 10 epochs,
    and evaluates how well the hyperparameters performed.
    :param model_name: Name of the CNN architecture to evaluate hyperparameters on.
    :param hyperparameter_dict: Hyperparameter space dictionary generated by generate_hyperparameters() function.
    :return:
    """

    print("Running Hyperparameter optimisation loop for: " + str(model_name))
    validation_metrics = []
    for i in range(len(hyperparameter_dict["learning_rates"])):
        data_loaders, classes = load_dataset_and_transforms(
            "../iss_image_data/experiment3/",
            uses_inception=False,
            augment=True,
            batch_size=hyperparameter_dict["batch_sizes"][i],
        )

        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        model, input_size = initialise_model(model_name, len(classes), freeze_all=False)
        parameters_to_learn = get_parameters_to_learn(model, training_mode="finetuning")
        model = model.to(device)
        optimizer = optim.Adam(
            parameters_to_learn,
            lr=hyperparameter_dict["learning_rates"][i],
            weight_decay=hyperparameter_dict["weight_decays"][i],
        )
        criterion = nn.CrossEntropyLoss()

        print("\nStarting iteration " + str(i) + " with Hyperparameters: \n")
        print("Learning Rate: " + str(hyperparameter_dict["learning_rates"][i]))
        print("Batch Size: " + str(hyperparameter_dict["batch_sizes"][i]))
        print("Weight Decay: " + str(hyperparameter_dict["weight_decays"][i]))
        model, history = train_model(
            model,
            data_loaders,
            device,
            criterion,
            optimizer,
            model_name,
            uses_inception=False,
            epochs=15,
        )
        validation_metrics.append(np.max(history["val_acc"]))

    print(
        "Best validaiton accuracy was achieved at iteration: "
        + str(np.argmax(validation_metrics))
    )
    print("Best performing hyperparameters were: \n")
    print(
        "Learning Rate: "
        + str(hyperparameter_dict["learning_rates"][np.argmax(validation_metrics)])
    )
    print(
        "Batch Size: "
        + str(hyperparameter_dict["batch_sizes"][np.argmax(validation_metrics)])
    )
    print(
        "Weight Decay: "
        + str(hyperparameter_dict["weight_decays"][np.argmax(validation_metrics)])
    )

    return


def main():

    plt.style.use("ggplot")

    hyperparameters = generate_hyperparameters(100)
    set_up_training_loops(sys.argv[1], hyperparameter_dict=hyperparameters)

    return


if __name__ == "__main__":
    main()
